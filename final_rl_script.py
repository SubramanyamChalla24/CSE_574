# -*- coding: utf-8 -*-
"""planning_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10jz1emrtwIWe0uw2OlMYU_A82mD2H2Su
"""

!pip install openai

!pip install stable_baselines3

!pip install transformers

!pip install huggingface_hub
!pip install bitsandbytes
!pip install datasets
!pip install peft
!pip install accelerate

"""Prompt generation using LLM"""

import json
import os
from pprint import pprint
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
#from datasets import load_dataset
from huggingface_hub import notebook_login
from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training
)
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
#notebook_login()

MODEL_NAME = "jeong-jasonji/CSE574_prompter"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

generation_config = model.generation_config
generation_config.max_new_tokens = 200
generation_config.temperature = 0.1
generation_config.top_p = 0.7
generation_config.num_return_sequences = 1
generation_config.pad_token_id = tokenizer.eos_token_id
generation_config.eos_token_id = tokenizer.eos_token_id

device = "cuda:0"

from transformers import AutoModelForCausalLM, AutoTokenizer
import huggingface_hub


def generate_prompt_transformer(company, strategy):

    action = "make a convincing ad for "+ company + " with themes of " + strategy
    encoding = tokenizer(action, return_tensors="pt").to(device)
    with torch.inference_mode():
      outputs = model.generate(
          input_ids = encoding.input_ids,
          attention_mask = encoding.attention_mask,
          generation_config = generation_config,
          max_new_tokens = 77
      )
      prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)
      return prompt


#prompt = generate_prompt_transformer("h&m","elegance")
#1
prompt

"""Manual prompt generation based on persuasion strategies and product specifics

import openai
openai.api_key = "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"

def generate_image_prompt_without_llm(persuasion_strategy, product_specifics):

  prompt = f"Generate an image without text for a {product_specifics['product_name']} commercial using the {persuasion_strategy} persuasion strategy. The image should highlight the following key features and benefits of the product:\n"
  for feature in product_specifics['features']:
    prompt += f"* {feature}\n"

  return prompt

product_specifics = {
    "product_name": "iPhone 14",
    "features": ["new camera system", "faster processor", "longer battery life"],
}

persuasion_strategy = "emotional"

prompt = generate_image_prompt_without_llm(persuasion_strategy, product_specifics)

print(prompt)

Image generation from prompt
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!pip install openai
!pip install diffusers

!pip -qqq install bitsandbytes accelerate
!pip install peft==0.6.2

import openai
import time
import requests
import os
import torch
from diffusers import StableDiffusionXLPipeline
from PIL import Image

pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
)
pipe = pipe.to("cuda")
num_images = 5
# Function to generate images based on a prompt
def prompt_to_image(prompt, directory="images"):

    for i in range(num_images):

        image = pipe(prompt).images[0]

        obj_directory = os.path.join(directory)
        if not os.path.exists(obj_directory):
            os.makedirs(obj_directory)

        image.save(f"{obj_directory}/{prompt[:5].replace(' ', '_')}_{i}.png", format="PNG")
        print(f"Image {i} for prompt '{prompt}' successfully saved.")

#prompt_to_image(prompt)

"""feedback loop"""

import statistics

def get_human_feedback(num_images):
  scores = []
  for i in range(0,num_images):
     scores.append((int)(input("Enter your feedback on a scale of -5 to 5 for image{}.png ".format(i))))
  average = statistics.mean(scores)
  return average

import base64

# Define a function for getting AI feedback on images
def get_ai_feedback(image_path):
    try:
        with open(image_path, 'rb') as image_file:
            image_content = base64.b64encode(image_file.read()).decode('utf-8')
            response = openai.Image.create(files=[{"data": {"base64": image_content}}], model="clip-vit-base-patch16")
            feedback = response.output.text
            return int(feedback)
    except Exception as e:
        print(f"Error: {e}")
        return None

# Example usage
image_path = '/content/image0.png'  # Provide the path to your image file
ai_feedback = get_ai_feedback(image_path)
if ai_feedback is not None:
    print(f"AI feedback for the image is: {ai_feedback}")

def get_ai_feedback_rating_for_image_link(image_link):

  feedback = openai.Feedback.create(
      feedback_options={"ai": True},
      file=image_link
  )

  return feedback["ratings"]["ai"]

"""RL model training for selecting persuasive strategies (still working on it)"""

import gym
import stable_baselines3
import requests


class AdsEnv(gym.Env):
    def __init__(self, persuasion_strategies, max_timesteps=3):
        self.persuasion_strategies = persuasion_strategies
        self.state_space = gym.spaces.Discrete(len(self.persuasion_strategies) ** 3)
        self.action_space = gym.spaces.Discrete(len(self.persuasion_strategies))
        self.state = None
        self.image = None
        self.observation_space = self.state_space
        self.selected_persuasion_strategies = set()
        self.max_timesteps = max_timesteps


    def reset(self):
      self.state = 0
      self.image = None
      self.selected_persuasion_strategies.clear()
      self.timestep = 0
      return self.state

    def step(self, action):

        self.timestep += 1
        persuasion_strategy_indices = [action % (len(self.persuasion_strategies))]
        persuasion_strategies = [self.persuasion_strategies[i] for i in persuasion_strategy_indices]

        if len(self.selected_persuasion_strategies) != len(persuasion_strategies):
            reward = -2
        else:
            reward = 2

        for persuasion_strategy in persuasion_strategies:
          self.selected_persuasion_strategies.add(persuasion_strategy)

        print("persuasion strategy selected is:{}".format(persuasion_strategies))

        reward += get_human_feedback(5)

        self.state = reward
        done = self.timestep >= self.max_timesteps

        return self.state, reward, done, {}

    def render(self, mode="human"):
        if mode == "human":
          # Display the current image
          self.image.show()
        elif mode == "rgb_array":
          # Return an RGB array of the current image
          return self.image.convert("RGB")

persuasion_strategies = ["Liking", "Social Proof", "Scarcity", "Authority", "Consistency", "Reciprocity", "Contrast", "Unity", "Consensus", "Emotion", "Scarcity", "Urgency", "Exclusivity", "Curiosity", "Anticipation", "Mystery", "Surprise", "Humor"]
env = AdsEnv(persuasion_strategies,3)
#rgb_array = env.render(mode="rgb_array")

!pip install shimmy>=0.2.1

agent = stable_baselines3.PPO('MlpPolicy', env)
agent.learn(total_timesteps=1)